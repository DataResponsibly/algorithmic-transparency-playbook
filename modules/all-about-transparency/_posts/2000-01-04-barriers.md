---
title: Becoming a Transparency Influencer
---

## Algorithms that exist or are being built
_Suggested time: 15 minutes_

Why don't organizations maek algorithmic transparency a priority?

There are several key reasons why organizations avoid or neglect algorithmic transparency. In this section, we disccuss these reasons, and importantly offer a rebuttal as to why they are inadequate justifaction for not pursuing transparency. These rebuttals can be useful to help you become a **transparency influencer** within your organization, and create meaningful change towards having a more transparent, ethical approach to algorihtms.

<br>

### Claim: implementing transparency means sacrificng accuracy

**This is not true.**

Many managers are concerned that implementing transparency means reducing the sophistication of their algorithmic systems, thereby decreasing their efficiency and accuracy. For example, in the context of algorithmic hiring, some managers erroneously believe that making the resume screener more transparent means it will perform worse.

However, recent research has challenged this idea. First, as described previously, there are a number of transparency tools that can be used to "open up" even the most complex, sophisticated black-box systems. Second, there is a growing number of case studies showing that under many conditions, simpler, more transparent algorithmic systems can perform the same (or even better than) complex systems. **Overall, implementing transparency does not necessarily result in sacrificing efficiency–it’s not that simple!**

<br>

### Claim: transparency means more costs

While its true that are some costs to implementing algorithmic transparency, **these costs are often grossly overstated --- especially when compared to the potential costs of _not_ implementing transparency.** Later in this course, we will detail the exact process and types of conversations one needs to have to implement transparency.

**Notably, what is often _understated_ is the costs that are saved through algorithmic transparency.** Transparency can be used to avoid performance risks like algorithmic errors, security risks, control risks like rogue outcomes and unintended consequences, economic risks, ethical risks, and societal risks like unfair outcomes for underprivileged or marginalized communities. All of these risks can have costly consequneces, like poor algorithms leading to worse business decisions or public relations risks, which can be greatly mitigated by using transpranecy.

> **Case Study: The Cost of Not Being Transparent** <br><br> In 2019, Meta (then Facebook) recieved a record-breaking FTC fine of $5 billion for violations related to privacy, accountability and transparency. Despite the fine being substantial, the true penalty is that Meta is now required to hire compliance officers that activetely participate and oversee business decisions at _all levels of the company_ --- a cost that is even more impactful than the $5 billion fine. This case study illustrates a key point: *ignoring transparency may save costs in the short-run, but leaves an organization vulnerable to catastrophic risks in the future.*

<br>

### Claim: transparency means open-sourcing

Algorithmic transparency is **not** the same as ``open-sourcing'' technologies. While providing the source code for an algorithm does offer some transparency into how it works, *it is not necessary for transparency.*

In fact, in many cases, open-sourcing is an insufficinet or misguided attempt at transparency for two reasons: first, the source code is not useful for laymen or any non-technical stakeholders of the algorithm in helping them understand how it works. Second, the source code for an algorithm is only one component of a much larger technical ecosystem. Without the data that is used by the algorithm or the technical infrastructure that supports it, the source code may be completely useless.

Note that in some situations open-sourcing _can_ be a component of transparency, but it is by no means required.

<br>

### Claim: transparency means a loss of Intellectual Property (IP)

Protecting IP is often a major concern for small startups or companies whose main competetive advantage is due to the IP of their algorithms. It would be untruthful for us to say that transparency is not, at least in some ways, in conflict with protecting IP.

However, we forward the claim that _it is possible_ to implement elements of transparency without significantly jeopardizing the privacy of an organization's IP. In some ways, there is a balancing act to perform between taking advantage of the benefits of algorithmic transparency, while protecting IP. We offer the following ideas for transparency when IP protection is also a consideration:

- *Creating transparency for different pieces of elements of your algorithm, where those sum of those peices are insufficient to full reconstruct your IP.* It is not uncommon that an algorithm is made up of multiple layers of decision making, or uses tens (or hundreds) of attributes, factors, and inputs to make decisions. Perhaps it is possible to implement transparency sourrundings specific layers of the algorithm, or focusing on just 3-5 factors (or broad categories of factors).
- *Using Differential Privacy.* Differential privacy is a system for publicly sharing information about a dataset by describing the patterns of groups within the dataset, while withholding the true information contained in a dataset.

<br>

### Claim: transparency means sacrificing privacy

Privacy in the context of algorithmic systems is generally concerned with protecting sensitive data that the organization has collected. We want to make it clear that **it is a complete myth** that one has to sacrafice the privacy of their data to offer transparency. It is _never_ necessary to expose sensitive or proprietary data to ensure algorithmic transparency. In fact, this is exactly the objective of *Differential Privacy.*

> **Differential privacy** is a large --- and sometimes comlex --- topic, but a primer can be found [here](https://privacytools.seas.harvard.edu/differential-privacy).

<br>

### Claim: transparency means our algorithms are vunerable to stratetigc manipulation by our users (being gamed)

Unfortunately, even without algorithmic transparency, strategic manipulation of algorithms by users is widespread. For example, in the hiring space, some candidates add invisible keywords in white text to the bottom of their resumes to trick algorithms into scoring their application higher. Another more prevelant example is Search Engine Opmitization (SEO), which is the process of trying to "game" your webpage to the top of the serach results.

In light of this, transparency may actually be an end-run around strategic manipulation. **There is substantial data showing that algorithmic transparency increases the trust of users.** It's not unreasonable to believe that if users trust a system, a good faith dialogue can be opened up about preventing strategic manipulation and the abuse of those systems.

<br>

## When algorithmic tools are or will be procured

Many organizations, especially in governmental or intergovernmental organizations, choose to procure their algorithmic tools instead of building them in-house. This posess a unique challenge, because because it may be beyond the agency or control of your orgnization to implement transparency for these tools. **For this reason _agency to implement transparency_ is an important consideration to weigh when choosing to procure algorithmic tools.**

In light of this, we have also drafted this list of probing questions that can be asked to organizations providing algorithmic tools that will help open the conversation around transparency:

- What are your values around algorithmic transparency?
- What considerations to transparency have you implemented in the tool being considered?
- If we require additional transparency considerations, are you able to implement them?
- What transparency is available to the organization _selling_ the tool, that is not available to the _procuring_ organization (and ultimately those are impacted by the tool)?
- How much transparency can we pass onwards to those impacted by the algorithmic tool?

> **Case Study: Model Cards for AI Transparency** <br><br> _Salesforce_ is a Fortune 500 company that is well known for selling software tools to business and non-profits. Notably, in 2020, they began producing [model cards](https://blog.salesforceairesearch.com/model-cards-for-ai-model-transparency/) for their algorithmic tools. **Model cards** provide high-level details about an underlying algorithm like _when and where it was created, its intended primary and secondary uses, what factors the algorithm considers, and against which metrics the performance of the tool was evaluated_.